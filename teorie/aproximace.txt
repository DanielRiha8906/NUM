Explanation for a High School Graduate

Linear Least Squares (OLS) - Using Gaussian Elimination

Linear least squares is used when the equation can be rewritten as a linear combination of unknown parameters. The goal is to find the best values for these parameters so that the function closely matches the given data.

Setting Up the ProblemWe have an equation of the form:

y = x + a b + a^2 b

If one of the variables (like a) is known, we can treat the terms involving it (a and a^2) as separate variables and rewrite the equation in matrix form:

y = β_0 + β_1 x + β_2 a + β_3 a^2

where β_0, β_1, β_2, β_3 are unknowns we want to estimate.

Building the Design MatrixWe organize the known data into a matrix:

X =[ 1  x_1  a_1  a_1^2 ][ 1  x_2  a_2  a_2^2 ]...[ 1  x_n  a_n  a_n^2 ]

and a vector for y:

Y =[ y_1 ][ y_2 ]...[ y_n ]

The equation we need to solve is:

X^T X β = X^T Y

Solving for Coefficients Using Gaussian Elimination

Forward elimination: Transform the system into an upper triangular form.

Back substitution: Solve for the unknowns starting from the last row.

Computing the Fit and ErrorAfter finding the coefficients, we compute the predicted values and measure the error using Mean Squared Error (MSE):

MSE = (1/n) Σ (y_i - ŷ_i)^2

This tells us how well our model fits the data.

Nonlinear Least Squares - Using Gradient Descent

If both a and b are unknown, the equation is nonlinear, meaning we must iteratively improve guesses.

Understanding the Error FunctionWe define an error function that measures how far our equation is from the actual data:

S(a, b) = Σ (y_i - (x_i + a b + a^2 b))^2

Using Gradient Descent

Compute the partial derivatives of S(a, b) with respect to a and b.

Update a and b in the opposite direction of the gradient:

a = a - α (∂S/∂a),   b = b - α (∂S/∂b)

where α (learning rate) controls the step size.

Stopping Conditions

If updates become very small.

If we reach a set number of iterations.

Final ValuesAfter many iterations, we get the best estimates for a and b.

Explanation for Someone Familiar with Numerical Methods

Linear Least Squares - Solving Normal Equations Without Built-In Solvers

The equation:

y = x + a b + a^2 b

is linear in parameters if a is known. The least squares estimate is found by solving:

X^T X β = X^T Y

Computational Approach

Compute X^T X and X^T Y explicitly.

Solve the system using Gaussian elimination:

Pivoting: Ensure numerical stability by swapping rows.

Forward elimination: Convert X^T X into an upper triangular form.

Back substitution: Solve for β.

Complexity Considerations

X^T X is symmetric and positive semi-definite.

Gaussian elimination has a complexity of O(n^3).

Nonlinear Least Squares - Gradient Descent Approach

Since both a and b are unknown, the problem is nonlinear in parameters. We define the objective function:

S(a, b) = Σ (y_i - (x_i + a b + a^2 b))^2

Gradient Computation

The gradients are:

∂S/∂a = -2 Σ (y_i - (x_i + a b + a^2 b)) (b + 2 a b)

∂S/∂b = -2 Σ (y_i - (x_i + a b + a^2 b)) (a + a^2)

Optimization Strategy

Use gradient descent with a fixed learning rate α.

Update parameters:

a = a - α (∂S/∂a),   b = b - α (∂S/∂b)

Stop when the gradient norm is below a tolerance level.

Stability Considerations

Step size selection: A too-large α may cause divergence.

Hessian-based approaches: More advanced methods (e.g., Levenberg-Marquardt) could be used.

Complexity Analysis

Each iteration requires O(n) operations.

Convergence depends on step size and initial values.

Final Thoughts

Method

Type

Solution Approach

Complexity

OLS (Linear Least Squares)

Linear in parameters

Gaussian elimination on X^T X β = X^T Y

O(n^3)

Nonlinear Least Squares

Nonlinear in parameters

Gradient descent on sum of squared residuals

O(n) per iteration